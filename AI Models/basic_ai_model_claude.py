# -*- coding: utf-8 -*-
"""Basic-AI-Model-CLAUDE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16ncM1fNeFMvulVVk3Mhx5KrJ5r1ib8yP
"""

!pip install ctransformers
!pip install transformers
!pip install autoawq

'''from ctransformers import AutoModelForCausalLM

# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.
#jitna zyada GPU accellaration utna zyada gpu layer
llm = AutoModelForCausalLM.from_pretrained("TheBloke/claude2-alpaca-13B-GGUF", model_file="claude2-alpaca-13b.Q4_K_M.gguf", model_type="llama", gpu_layers=50)

print(llm("5 blog topic ideas on subscription economy"))'''

#THe above code was working fine with claude gguf model / the below claude AWQ model requires GPU accellaration to run.

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="TheBloke/claude2-alpaca-13B-AWQ")

def get_completion_dolly(input):
  system = f"""
  You are an expert in Content Marketing.
  You are good at explaining Content Marketing concepts in simple words.
  Help as much as you can.
  """
  prompt = f"#### System: {system}\n#### User: \n{input}\n\n#### Response from claude2-alpaca-13B-AWQ:"
 #prompt = f"#### User: \n{input}\n\n#### Response from Dolly-v2-3b:"
  print(prompt)
  dolly_response = pipelines(prompt,
                                  max_new_tokens=500
                                  )
  return dolly_response[0]["generated_text"]